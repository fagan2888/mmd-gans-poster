%% beamerthemeImperialPoster v1.0 2016/10/01
%% Beamer poster theme created for Imperial College by LianTze Lim
%% LICENSE: LPPL 1.3
\documentclass[xcolor={table}]{beamer}
\usepackage[size=a0,orientation=landscape,scale=1.55]{beamerposter}
\setlength{\oddsidemargin}{1in}
\usepackage{wrapfig}
\usepackage{amsmath}
\usepackage{tabu}
\usepackage{booktabs}
%\usepackage{subcaption}
\usepackage{subfig}[caption=False]
% \captionsetup[table]{font={stretch=1.2}}     %% change 1.2 as you like

\usetheme{ImperialPoster}
%% Four available colour themes
\usecolortheme{ImperialWhite} % Default
% \usecolortheme{ImperialLightBlue}
% \usecolortheme{ImperialDarkBlue}
% \usecolortheme{ImperialBlack}

\title{Demystifying MMD GANs}
\author{
  \mainauthor{Miko{\l}aj Bi\'nkowski}\Tsup{1}, 
  \mainauthor{Dougal J. Sutherland}\Tsup{2},
  Arthur Gretton\Tsup{2},
  Michael Arbel\Tsup{2}
}
\institute{
  \Tsup{1}Department of Mathematics, Imperial College London, 
  \Tsup{2}Gatsby Computational Neuroscience Unit, University College London\\
  \texttt{\{mikbinkowski,dougal,arthur.gretton,michael.n.arbel\}@gmail.com}
}

\DeclareMathOperator{\D}{\mathcal{D}}
\DeclareMathOperator*{\E}{\mathbb{E}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\h}{\mathcal{H}}
\DeclareMathOperator{\mean}{mean}
\newcommand{\PP}{\mathbb P}
\newcommand{\QQ}{\mathbb Q}
\newcommand{\R}{\mathbb R}
\DeclareMathOperator{\W}{\mathcal{W}}
\newcommand{\X}{\mathcal X}
\newcommand{\Z}{\mathcal Z}
\newcommand{\ZZ}{\mathbb Z}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator{\mmd}{MMD}
\DeclareMathOperator{\mmdhat}{\widehat{MMD}}

\addbibresource{refs.bib}

\begin{document}
\addtobeamertemplate{headline}{} % Imperial Logo. This is a bit hacky
{
\begin{tikzpicture}[remember picture,overlay] 
\node [shift={(-11cm,-5cm)}] at (current page.north east) {\includegraphics[height=4.5cm]{Imperial_logo.pdf}}; 
\end{tikzpicture} 
\begin{tikzpicture}[remember picture,overlay]
\node [shift={(11cm,-5cm)}] at (current page.north west) {\includegraphics[height=11cm]{gatsby.jpg}};
\end{tikzpicture}
}


\begin{frame}{}
\maketitle
\begin{columns}[T, totalwidth=\textwidth]

  \begin{column}{.32\textwidth}
    \begin{block}{Introduction}
      We investigate the training and performance of generative adversarial networks 
      using the Maximum Mean Discrepancy (MMD GANs). We also clarify the situation with 
      bias in GAN loss functions raised by recent work and relation between MMD GANs and Cram\'er GAN.
      Finally, we propose new measure of GAN performance, the \emph{Kernel Inception Distance} and show
      how to use it to dynamically adapt learning rates during GAN training.
    \end{block}

    \begin{block}{Relation to Wasserstein and Cram\'er GANs} 
      Integral Probablity Metrics (IMPs) are a family of divergences between 
      probability distribution
      \begin{equation}
        \D(\PP, \QQ) = \sup_{f\in\F} \E_{X\sim\PP}f(X) - \E_{Y\sim\QQ}f(Y),
      \end{equation}
      where $\F$ is some class of \emph{critic} functions. 
      \begin{itemize}
        \item{\textbf{Wasserstein distance} is the IPM with $\F$ being the set of 1-Lipschitz functions
          \[  
            \F = \left\{f: \sup\frac{|f(x) - f(y)|}{\|x - y\|}\leq 1\right\}. 
          \]
          Wasserstein GANs approximate this class via critic network, with Lipschitz 
          constraint enforced through weight clipping \citep{wgan} or gradient 
          penalty \citep{wgan-gp}
        \item \textbf{Maximum Mean Discrepancy (MMD)} \citep{mmd-jmlr} is the IPM 
          where $\F$ is the unit ball in some 
          \emph{Reproducing Kernel Hilbert Space (RKHS)}
          \[ \F = \left\{f: \|f\|_{\h} \leq 1\right\}. \]
          Value of MMD and the optimal critic are known in the closed form
          \begin{align*}
            MMD(\PP, \QQ) &= \E_{\PP} k(X,X') + \E_{\QQ} k(Y,Y') - 2\E_{\PP,\QQ} k(X,Y),\\
            f^*(t) &= \E_{\PP}k(X, t) - \E_{\QQ}k(Y, t),
          \end{align*}
          Where $k$ is a kernel function. MMD GANs \citep{mmd-gan}, however, 
          optimize the representation in the composite kernel 
          \[ k_{\theta}(x, y) = k_{base}(h_{\theta}(x), h_{\theta}(y)) \].
        }
        \item Cram\'er GAN is equivalent to MMD GAN with \emph{Energy Distance} kernel.
        \item WGAN can be seen as  MMD GAN with linear kernel.
      \end{itemize}
    \end{block}
  \end{column}

  \begin{column}{.32\textwidth}
    \begin{block}{Gradient Penalty}
      We propose to extend the idea of gradient penalty to MMD-GANs by penalizing the
      gradient of the witness function 
      \[ Loss^{critic} = \widehat{MMD_{\theta}}(\PP, \QQ_{\psi}) + \lambda\E_{\tilde{X}}\left(\|\nabla_{\tilde{X}} f^*(\tilde{X})\| - 1\right)^2, \]
      where $\widehat{MMD}$ is an unbiased MMD estimator and $\QQ_{\psi} = G_{\psi}(\ZZ)$ is the generated distribution.
    \end{block}
    \vspace*{-1cm}
    \begin{block}{Biased gradient estimates}
      \citet{cramer-gan} claim that WGANs have biased gradients, while Cram\'er GANs do not. 
      As a theoretical contribution, we prove that for Wasserstein, Cram\'er and MMD GANs gradient estimates
      are unbiased, but learning a discriminator based on samples leads to biased gradients 
      for the generator parameters. Precisely, $\widehat{\D}$ is the estimate of the loss function 
      \begin{itemize} 
        \item $\nabla_{\psi}\widehat{\D}(X, G_{\psi}(Z))$ is biased,
        \item for any fixed $\theta$, $\nabla_{\psi, \theta} \widehat{\D}(X, G_{\psi}(Z))$ is unbiased.
      \end{itemize}
    \end{block}
    \vspace*{-1cm}
    \begin{block}{Results}
      MMD GANs with gradient penalty and the right kernel lead to slightly or significantly better results 
      than WGAN-GP. We recommend use of a mixture of rational-quadratic kernels.
      Use of MMD as a loss fuction in training GANs also allows limiting the size of the critic 
      network (while preserving sample quality). 
    \end{block}
    \vspace*{-1cm}
    \begin{figure}
      \centering
      \includegraphics[width=.32\columnwidth]{samples/celeba-mmd-rq-25.png}
      ~
      \includegraphics[width=.32\columnwidth]{samples/celeba-wgan-25.png}
      ~
      \includegraphics[width=.32\columnwidth]{samples/celeba-cramer-25.png}
      \caption{Samples for \textbf{$160 \times 160$ CelebA} dataset trained with 
               MMD GAN (left) and WGAN-GP (right) with ResNet generator and DCGAN discriminator.}
    \end{figure}
%    \begin{sidefigure}
%      \centering
%      \includegraphics[width=.5\columnwidth]{samples/lsun_rq_16.png}
%      ~
%      \includegraphics[width=.5\columnwidth]{samples/lsun_wgan_16.png}
%      \caption{Samples for \textbf{$64 \times 64$ LSUN bedroom} dataset trained with 
%               MMD GAN (left) and WGAN-GP (right). Models trained with DCGAN 
%               architecture with \emph{smaller} discriminator.}
%    \end{sidefigure}
    \vspace*{-1cm}
    \begin{table}
      \centering
      \caption{Mean (standard deviation) of score evaluations for the CelebA dataset.}
      \label{tab:celeba-scores}
      \begin{tabular}{cc|rrr}
        loss & top layer & \multicolumn{1}{c}{Inception} & \multicolumn{1}{c}{FID} & \multicolumn{1}{c}{KID} \\
        \hline
        MMD RQ   &   16 &    2.61  (0.01) &   20.55  (0.25) &   0.013  (0.001)\\
        Cram\'er &  256 &    2.86  (0.01) &   31.30  (0.17) &   0.025  (0.001)\\
        WGAN-GP  & 1    &    2.72  (0.01) &   29.24  (0.22) &   0.022  (0.001)\\
        test set & --   &    3.76  (0.02) &    2.25  (0.04) &   0.000  (0.000)\\
    \end{tabular}
\end{table}
  \end{column}

  \begin{column}{.32\textwidth}
    \begin{block}{KID - new evaluation method for GANs}
      \emph{Inception score} and \emph{Fr\'echet Inception Distance (FID)} are two commonly used methods 
      of quantitative evalutaion of GANs. Yet, both have drawbacks. Inception is 
      independent of the target distribution and not meaningful for datasets such as LSUN or CelebA.
      Although FID addresses these issues, its estimates are biased, as we show in this work.

      For that reason, we propose an alternative method, \emph{Kernel Inception Distance (KID)}, defined as 
      MMD estimate (with polynomial kernel $k(x,y) = \left(\frac{1}{d}\langle x, y\rangle + 1\right)^3$
      between Inception hidden layer activations of the real and generated samples. 
    \end{block}
    \begin{figure}
      \centering
      \includegraphics[width=.48\columnwidth]{figs/mmd-unbiased.pdf}\quad
      \includegraphics[width=.48\columnwidth]{figs/fid-bias.pdf}
      \caption{Estimates of KID (left) and FID (right) between the CIFAR-10 train and test sets. FID estimates 
        exhibit strong bias for $n$ even up to $10\,000$, which is not the case for KID. 
        Standard deviation of estimates shrinks quickly for KID and is always small for FID.}  %Each point is based on 100 samples, estimating with replacement; sampling without replacement, and/or using the full training set, gives similar results. Lines show means, error bars standard deviations, dark colored regions a $\frac{2}{3}$ coverage interval of the samples, light colored regions a $95\%$ interval. Note the differing $n$ axes.}
      \label{fig:fid-kid-bias}
    \end{figure}
    \begin{block}{Learning Rate Adaptation}
      Decreasing learning rate during training is a common practice in deep learning, 
      so far not explored in GAN setting. Instead of tuning learning rate by hand, we propose 
      using an adaptive scheme, based on comparing the KID score for samples from a previous 
      iteration to that from the current iteration.
    \end{block}
     \printbibliography
  \end{column}

\end{columns}


\end{frame}


\end{document}
