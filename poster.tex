%% beamerthemeImperialPoster v1.0 2016/10/01
%% Beamer poster theme created for Imperial College by LianTze Lim
%% LICENSE: LPPL 1.3
\documentclass[xcolor={table}]{beamer}
\usepackage[size=a0,orientation=landscape,scale=1.55]{beamerposter}
\setlength{\oddsidemargin}{1in}
\usepackage{wrapfig}
\usepackage{amsmath}
\usepackage{tabu}
\usepackage{booktabs}
%\usepackage{subcaption}
\usepackage{subfig}[caption=False]
% \captionsetup[table]{font={stretch=1.2}}     %% change 1.2 as you like

\usetheme{ImperialPoster}
%% Four available colour themes
\usecolortheme{ImperialWhite} % Default
% \usecolortheme{ImperialLightBlue}
% \usecolortheme{ImperialDarkBlue}
% \usecolortheme{ImperialBlack}

\title{Demystifying MMD GANs}
\author{
  \mainauthor{Miko{\l}aj Bi\'nkowski}\Tsup{1}, 
  \mainauthor{Dougal J. Sutherland}\Tsup{2},
  Arthur Gretton\Tsup{2},
  Michael Arbel\Tsup{2}
}
\institute{
  \Tsup{1}Department of Mathematics, Imperial College London, 
  \Tsup{2}Gatsby Computational Neuroscience Unit, University College London\\
  \texttt{\{mikbinkowski,dougal,arthur.gretton,michael.n.arbel\}@gmail.com}
}

\DeclareMathOperator{\D}{\mathcal{D}}
\DeclareMathOperator*{\E}{\mathbb{E}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\h}{\mathcal{H}}
\DeclareMathOperator{\mean}{mean}
\newcommand{\PP}{\mathbb P}
\newcommand{\QQ}{\mathbb Q}
\newcommand{\R}{\mathbb R}
\DeclareMathOperator{\W}{\mathcal{W}}
\newcommand{\X}{\mathcal X}
\newcommand{\Z}{\mathcal Z}
\newcommand{\ZZ}{\mathbb Z}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator{\mmd}{MMD}
\DeclareMathOperator{\mmdhat}{\widehat{MMD}}

\addbibresource{refs.bib}

\begin{document}
\addtobeamertemplate{headline}{} % Imperial Logo. This is a bit hacky
{
\begin{tikzpicture}[remember picture,overlay] 
\node [shift={(-11cm,-5cm)}] at (current page.north east) {\includegraphics[height=4.5cm]{Imperial_logo.pdf}}; 
\end{tikzpicture} 
\begin{tikzpicture}[remember picture,overlay]
\node [shift={(11cm,-5cm)}] at (current page.north west) {\includegraphics[height=11cm]{gatsby.jpg}};
\end{tikzpicture}
}


\begin{frame}{}
\maketitle
\begin{columns}[T, totalwidth=\textwidth]

  \begin{column}{.32\textwidth}
    \begin{block}{Introduction}
      We investigate the training and performance of generative adversarial networks 
      using the Maximum Mean Discrepancy (MMD GANs). We also clarify the situation with 
      bias in GAN loss functions raised by recent work and relation between MMD GANs and Cram\'er GAN.
      Finally, we propose new measure of GAN performance, the \emph{Kernel Inception Distance} and show
      how to use it to dynamically adapt learning rates during GAN training.
    \end{block}
    \vspace*{-1.2cm}
    \begin{block}{Relation to Wasserstein and Cram\'er GANs} 
      Integral Probablity Metrics (IPMs) are a family of divergences between 
      probability distribution
      \begin{equation}
        \D(\PP, \QQ) = \sup_{f\in\F} \E_{X\sim\PP}f(X) - \E_{Y\sim\QQ}f(Y),
      \end{equation}
      where $\F$ is some class of \emph{critic} functions. 
      \begin{itemize}
        \item{\textbf{Wasserstein distance} is the IPM with $\F$ being the set of 1-Lipschitz functions
          \[  
            \F = \left\{f: \sup\frac{|f(x) - f(y)|}{\|x - y\|}\leq 1\right\}. 
          \]
          Wasserstein GANs approximate this class via critic network, with Lipschitz 
          constraint enforced through weight clipping \citep{wgan} or gradient 
          penalty \citep{wgan-gp}
        \item \textbf{Maximum Mean Discrepancy (MMD)} \citep{mmd-jmlr} is the IPM 
          where $\F$ is the unit ball in some 
          \emph{Reproducing Kernel Hilbert Space (RKHS)}
          \[ \F = \left\{f: \|f\|_{\h} \leq 1\right\}. \]
          MMD value and the optimal critic are known in the closed form
          \begin{align*}
            MMD(\PP, \QQ) &= \E_{\PP} k(X,X') + \E_{\QQ} k(Y,Y') - 2\E_{\PP,\QQ} k(X,Y),\\
            f^*(t) &= \E_{\PP}k(X, t) - \E_{\QQ}k(Y, t),
          \end{align*}
          Where $k$ is a kernel function. MMD GANs \citep{mmd-gan}, however, 
          optimize the representation in the composite kernel 
          \[ k_{\theta}(x, y) = k_{base}(h_{\theta}(x), h_{\theta}(y)). \]
        }
        \item Cram\'er GAN is MMD GAN with \emph{Energy Distance} kernel.
        \item WGAN can be seen as MMD GAN with linear kernel.
      \end{itemize}
    \end{block}
    \vspace*{-1.2cm}
    \begin{block}{Gradient Penalty}
      We propose to extend the idea of gradient penalty to MMD-GANs by penalizing the
      gradient of the witness function 
      \[ Loss^{critic}(\theta) = \widehat{MMD_{\theta}}(\PP, \QQ_{\psi}) + \lambda\E_{\tilde{X}}\left(\|\nabla_{\tilde{X}} f^*(\tilde{X})\| - 1\right)^2, \]
      where $\tilde{X}$ are drawn between points from $\PP$ and $\QQ_{\psi}$. %$\widehat{MMD}$ is an unbiased MMD estimator.% and $\QQ_{\psi} = G_{\psi}(\ZZ)$ is the generated distribution.
    \end{block}
  \end{column}

  \begin{column}{.32\textwidth}
    \begin{figure}
      \centering
      \includegraphics[width=.35\columnwidth]{figs/witness.png}
      ~
      \includegraphics[width=.35\columnwidth]{figs/trained_witness.png}
      \caption{Theoretical (left) and estimated (right) witness functions for MMD and Wasserstein distance between two Gaussians}
    \end{figure}
    \vspace*{-1.3cm}
    \begin{block}{Biased gradient estimates}
      \begin{itemize}
        \item \citet{cramer-gan} claim that WGANs have biased gradients, while Cram\'er GANs do not.
        \item We prove that for Wasserstein, Cram\'er and MMD GANs gradient estimates
              are unbiased, but learning a discriminator based on samples leads to biased gradients 
              for the generator parameters. If $\widehat{\D}$ is the estimate of the loss function: 
        \item $\nabla_{\psi}\widehat{\D}(X, G_{\psi}(Z))$ is biased,
        \item for any fixed $\theta$, $\nabla_{\psi, \theta} \widehat{\D}(X, G_{\psi}(Z))$ is unbiased.
      \end{itemize}
    \end{block}
    \vspace*{-1.3cm}
    \begin{block}{Results}
      MMD GANs with gradient penalty and right kernel lead to better results 
      than WGAN-GP and allow limiting the size of the critic
      network, while preserving sample quality. %We recommend use of a mixture of rational-quadratic kernels.
%      MMD GANs also allow limiting the size of the critic 
%      network, while preserving sample quality. 
    \end{block}
    \vspace*{-1.3cm}
%    \begin{figure}
%      \centering
%      \includegraphics[width=.32\columnwidth]{samples/celeba-mmd-rq-25.png}
%      ~
%      \includegraphics[width=.32\columnwidth]{samples/celeba-wgan-25.png}
%      ~
%      \includegraphics[width=.32\columnwidth]{samples/celeba-cramer-25.png}
%      \caption{Samples for \textbf{$160 \times 160$ CelebA} dataset trained with 
%               MMD GAN (left) and WGAN-GP (right) with ResNet generator and DCGAN discriminator.}
%    \end{figure}
    \begin{sidefigure4}
      \centering
      \includegraphics[width=.49\columnwidth]{samples/celeba-mmd-rq-25.png}
      ~
      \includegraphics[width=.49\columnwidth]{samples/celeba-wgan-25.png}
      \vspace*{10pt}
      \includegraphics[width=.49\columnwidth]{samples/lsun_rq_16.png}
      ~
      \includegraphics[width=.49\columnwidth]{samples/lsun_wgan_16.png}
      \caption{\textbf{$160 \times 160$ CelebA dataset.} \\
               Samples from
               MMD GAN (left) and WGAN-GP (right) trained with ResNet generator and DCGAN critic.\\
               \vspace*{95pt}
               \textbf{$64 \times 64$ LSUN bedroom dataset.}\\
               Samples from 
               MMD GAN (left) and WGAN-GP (right) models trained with DCGAN 
               architecture with \emph{small} critic.%\\[10pt]
               %MMD GANs trained with composite \emph{rational quadratic} kernel.
               }
    \end{sidefigure4}
    \vspace*{-1.3cm}
    \begin{table}
      \centering
      \vspace{-1cm}
      \caption{Mean (standard deviation) of score evaluations for the CelebA dataset.}
      \label{tab:celeba-scores}
      \begin{tabular}{cc|rrr}
        loss & top layer & \multicolumn{1}{c}{Inception} & \multicolumn{1}{c}{FID} & \multicolumn{1}{c}{KID} \\
        \hline
        MMD RQ   &   16 &    2.61  (0.01) &   20.55  (0.25) &   0.013  (0.001)\\
        Cram\'er &  256 &    2.86  (0.01) &   31.30  (0.17) &   0.025  (0.001)\\
        WGAN-GP  & 1    &    2.72  (0.01) &   29.24  (0.22) &   0.022  (0.001)\\
        test set & --   &    3.76  (0.02) &    2.25  (0.04) &   0.000  (0.000)\\
    \end{tabular}
\end{table}
  \end{column}

  \begin{column}{.32\textwidth}
    \begin{block}{KID - new evaluation method for GANs}
      \emph{Inception score} and \emph{Fr\'echet Inception Distance (FID)} are two commonly used methods 
      of quantitative evaluation of GANs. Yet, both have drawbacks. 
      \begin{itemize}
        \item Inception is independent of the target distribution and not meaningful for datasets such as LSUN or CelebA.
        \item FID estimates are biased.
        \item We propose an alternative method, \emph{Kernel Inception Distance (KID)}, defined as 
      MMD estimate (with polynomial kernel $k(x,y) = \left(\frac{1}{d}\langle x, y\rangle + 1\right)^3$
      between Inception hidden layer activations of the real and generated samples. 
        \item KID has low bias even for moderate sample sizes.
      \end{itemize}
    \end{block}
    \begin{figure}
      \centering
      \includegraphics[width=.48\columnwidth]{figs/mmd-unbiased.pdf}\quad
      \includegraphics[width=.48\columnwidth]{figs/fid-bias.pdf}
      \caption{Estimates of KID (left) and FID (right) between the CIFAR-10 train and test sets. FID estimates 
        exhibit strong bias for $n$ even up to $10\,000$, which is not the case for KID. 
        Standard deviation of estimates shrinks quickly for KID and is always small for FID.}  %Each point is based on 100 samples, estimating with replacement; sampling without replacement, and/or using the full training set, gives similar results. Lines show means, error bars standard deviations, dark colored regions a $\frac{2}{3}$ coverage interval of the samples, light colored regions a $95\%$ interval. Note the differing $n$ axes.}
      \label{fig:fid-kid-bias}
    \end{figure}
    \begin{block}{Learning Rate Adaptation}
      Decreasing learning rate during training is a common practice in deep learning, 
      so far not explored in GAN setting. Instead of tuning learning rate by hand, we propose 
      using an adaptive scheme, based on comparing the KID score for samples from a previous 
      iteration to that from the current iteration.
    \end{block}
     \printbibliography
  \end{column}

\end{columns}


\end{frame}


\end{document}
